/**
 * SubhutiParser æµ‹è¯• 003ï¼šOrè§„åˆ™é¡ºåºé—®é¢˜ï¼ˆå…³é”®æµ‹è¯•ï¼‰
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. éªŒè¯çŸ­è§„åˆ™åœ¨å‰ä¼šå¯¼è‡´é•¿è§„åˆ™æ— æ³•åŒ¹é…
 * 2. éªŒè¯é•¿è§„åˆ™åœ¨å‰èƒ½æ­£ç¡®å·¥ä½œ
 * 3. è¿™æ˜¯å¯¼è‡´ Slime Parser å¤±è´¥çš„æ ¹æœ¬åŸå› 
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createKeywordToken, createRegToken, createValueRegToken } from "../../src/struct/SubhutiCreateToken.ts"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken.ts"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  LetTok: createKeywordToken('LetTok', 'let'),
  AsTok: createKeywordToken('AsTok', 'as'),
  Eq: createRegToken('Eq', /=/),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  Spacing: createValueRegToken('Spacing', /[ \t]+/, ' ', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  LetTok() {
    return this.consume(testTokensObj.LetTok)
  }
  
  AsTok() {
    return this.consume(testTokensObj.AsTok)
  }
  
  Eq() {
    return this.consume(testTokensObj.Eq)
  }
  
  Identifier() {
    return this.consume(testTokensObj.Identifier)
  }
  
  Number() {
    return this.consume(testTokensObj.Number)
  }
}

// ============================================
// é”™è¯¯çš„Parserï¼ˆçŸ­è§„åˆ™åœ¨å‰ï¼‰
// ============================================

@Subhuti
class BadParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // âŒ é”™è¯¯ï¼šçŸ­è§„åˆ™åœ¨å‰
  @SubhutiRule
  ImportName() {
    this.Or([
      // çŸ­è§„åˆ™ï¼šåªåŒ¹é… "name"
      {
        alt: () => {
          this.tokenConsumer.Identifier()
        }
      },
      // é•¿è§„åˆ™ï¼šåŒ¹é… "name as userName"
      {
        alt: () => {
          this.tokenConsumer.Identifier()
          this.tokenConsumer.AsTok()
          this.tokenConsumer.Identifier()
        }
      }
    ])
  }
}

// ============================================
// æ­£ç¡®çš„Parserï¼ˆé•¿è§„åˆ™åœ¨å‰ï¼‰
// ============================================

@Subhuti
class GoodParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // âœ… æ­£ç¡®ï¼šé•¿è§„åˆ™åœ¨å‰
  @SubhutiRule
  ImportName() {
    this.Or([
      // é•¿è§„åˆ™ï¼šåŒ¹é… "name as userName"
      {
        alt: () => {
          this.tokenConsumer.Identifier()
          this.tokenConsumer.AsTok()
          this.tokenConsumer.Identifier()
        }
      },
      // çŸ­è§„åˆ™ï¼šåªåŒ¹é… "name"
      {
        alt: () => {
          this.tokenConsumer.Identifier()
        }
      }
    ])
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(60))
console.log('SubhutiParser æµ‹è¯• 003ï¼šOrè§„åˆ™é¡ºåºé—®é¢˜ï¼ˆå…³é”®æµ‹è¯•ï¼‰')
console.log('='.repeat(60))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šçŸ­è§„åˆ™åœ¨å‰ - åŒ¹é…çŸ­å½¢å¼
console.log('\n[æµ‹è¯•1] çŸ­è§„åˆ™åœ¨å‰ - åŒ¹é…çŸ­å½¢å¼: "name"')
try {
  const code1 = 'name'
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  console.log('  Token:', tokens1.map(t => t.tokenValue).join(' '))
  
  const parser1 = new BadParser(tokens1)
  const result1 = parser1.ImportName()
  
  if (result1 && result1.children.length > 0 && parser1.tokenIndex === 1) {
    console.log('  âœ… æˆåŠŸï¼šçŸ­å½¢å¼åŒ¹é…æ­£å¸¸')
    console.log('  æ¶ˆè´¹äº†', parser1.tokenIndex, 'ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šçŸ­è§„åˆ™åœ¨å‰ - åŒ¹é…é•¿å½¢å¼ï¼ˆä¼šå¤±è´¥ï¼‰
console.log('\n[æµ‹è¯•2] âŒ çŸ­è§„åˆ™åœ¨å‰ - å°è¯•åŒ¹é…é•¿å½¢å¼: "name as userName"')
console.log('  é¢„æœŸï¼šç¬¬ä¸€ä¸ªåˆ†æ”¯åŒ¹é… "name"ï¼Œå‰©ä½™ "as userName" æ— æ³•å¤„ç†')
try {
  const code2 = 'name as userName'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  console.log('  Token:', tokens2.map(t => t.tokenValue).join(' '))
  console.log('  Tokenæ•°é‡:', tokens2.length)
  
  const parser2 = new BadParser(tokens2)
  const result2 = parser2.ImportName()
  
  console.log('  è§£æå tokenIndex:', parser2.tokenIndex)
  console.log('  å‰©ä½™ token:', tokens2.length - parser2.tokenIndex, 'ä¸ª')
  
  if (result2 && parser2.tokenIndex === 1) {
    console.log('  âœ… ç¡®è®¤é—®é¢˜ï¼šåªæ¶ˆè´¹äº†ç¬¬ä¸€ä¸ªtokenï¼Œå‰©ä½™tokenæœªå¤„ç†')
    console.log('  è¿™å°±æ˜¯å¯¼è‡´ Slime Parser å¤±è´¥çš„åŸå› ï¼')
    passed++
  } else {
    console.log('  âŒ æ„å¤–ï¼šè¡Œä¸ºä¸ç¬¦åˆé¢„æœŸ')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šé•¿è§„åˆ™åœ¨å‰ - åŒ¹é…çŸ­å½¢å¼
console.log('\n[æµ‹è¯•3] é•¿è§„åˆ™åœ¨å‰ - åŒ¹é…çŸ­å½¢å¼: "name"')
try {
  const code3 = 'name'
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  console.log('  Token:', tokens3.map(t => t.tokenValue).join(' '))
  
  const parser3 = new GoodParser(tokens3)
  const result3 = parser3.ImportName()
  
  if (result3 && result3.children.length > 0 && parser3.tokenIndex === 1) {
    console.log('  âœ… æˆåŠŸï¼šç¬¬ä¸€ä¸ªåˆ†æ”¯å¤±è´¥ï¼ˆç¼ºå°‘asï¼‰ï¼Œå›æº¯åˆ°ç¬¬äºŒä¸ªåˆ†æ”¯æˆåŠŸ')
    console.log('  æ¶ˆè´¹äº†', parser3.tokenIndex, 'ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šé•¿è§„åˆ™åœ¨å‰ - åŒ¹é…é•¿å½¢å¼
console.log('\n[æµ‹è¯•4] âœ… é•¿è§„åˆ™åœ¨å‰ - åŒ¹é…é•¿å½¢å¼: "name as userName"')
try {
  const code4 = 'name as userName'
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  console.log('  Token:', tokens4.map(t => t.tokenValue).join(' '))
  console.log('  Tokenæ•°é‡:', tokens4.length)
  
  const parser4 = new GoodParser(tokens4)
  const result4 = parser4.ImportName()
  
  console.log('  è§£æå tokenIndex:', parser4.tokenIndex)
  console.log('  å‰©ä½™ token:', tokens4.length - parser4.tokenIndex, 'ä¸ª')
  
  if (result4 && result4.children.length > 0 && parser4.tokenIndex === 3) {
    console.log('  âœ… æˆåŠŸï¼šç¬¬ä¸€ä¸ªåˆ†æ”¯å®Œå…¨åŒ¹é…ï¼Œæ¶ˆè´¹äº†æ‰€æœ‰token')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser4.tokenIndex, 'ï¼ˆåº”è¯¥æ˜¯3ï¼‰')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(60))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(60))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(60))

console.log('\nğŸ“‹ å…³é”®ç»“è®ºï¼š')
console.log('1. Orè§„åˆ™æŒ‰é¡ºåºå°è¯•ï¼Œç¬¬ä¸€ä¸ªæˆåŠŸå³è¿”å›')
console.log('2. çŸ­è§„åˆ™åœ¨å‰ä¼šå¯¼è‡´é•¿è§„åˆ™æ°¸è¿œæ— æ³•åŒ¹é…')
console.log('3. å¿…é¡»å°†é•¿è§„åˆ™æ”¾åœ¨çŸ­è§„åˆ™å‰é¢ï¼')
console.log('4. è¿™æ˜¯ Slime Parser å¤±è´¥çš„æ ¹æœ¬åŸå› ')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}

