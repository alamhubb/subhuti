/**
 * SubhutiParser æµ‹è¯• 012ï¼šè¾¹ç•Œæƒ…å†µæµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. ç©ºè¾“å…¥å¤„ç†
 * 2. å•tokenè¾“å…¥
 * 3. è¶…é•¿è¾“å…¥
 * 4. æ·±åº¦åµŒå¥—ï¼ˆé˜²æ­¢æ ˆæº¢å‡ºï¼‰
 * 5. å¤§é‡é‡å¤è§„åˆ™
 * 6. EOFè¾¹ç•Œ
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createRegToken, createValueRegToken } from "../../src/struct/SubhutiCreateToken"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  LParen: createValueRegToken('LParen', /\(/, '('),
  RParen: createValueRegToken('RParen', /\)/, ')'),
  LBrace: createValueRegToken('LBrace', /{/, '{'),
  RBrace: createValueRegToken('RBrace', /}/, '}'),
  Comma: createValueRegToken('Comma', /,/, ','),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  LParen() { return this.consume(testTokensObj.LParen) }
  RParen() { return this.consume(testTokensObj.RParen) }
  LBrace() { return this.consume(testTokensObj.LBrace) }
  RBrace() { return this.consume(testTokensObj.RBrace) }
  Comma() { return this.consume(testTokensObj.Comma) }
  Semicolon() { return this.consume(testTokensObj.Semicolon) }
  Identifier() { return this.consume(testTokensObj.Identifier) }
  Number() { return this.consume(testTokensObj.Number) }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // å¯é€‰token
  @SubhutiRule
  OptionalToken() {
    this.Option(() => this.tokenConsumer.Identifier())
  }
  
  // å¤štokenåˆ—è¡¨
  @SubhutiRule
  TokenList() {
    this.Many(() => this.tokenConsumer.Identifier())
  }
  
  // åµŒå¥—æ‹¬å·
  @SubhutiRule
  NestedParens() {
    this.Or([
      {
        alt: () => {
          this.tokenConsumer.LParen()
          this.NestedParens()
          this.tokenConsumer.RParen()
        }
      },
      { alt: () => this.tokenConsumer.Identifier() }
    ])
  }
  
  // é•¿åˆ—è¡¨ï¼ˆæµ‹è¯•æ€§èƒ½ï¼‰
  @SubhutiRule
  LongList() {
    this.tokenConsumer.Identifier()
    this.Many(() => {
      this.tokenConsumer.Comma()
      this.tokenConsumer.Identifier()
    })
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(70))
console.log('SubhutiParser æµ‹è¯• 012ï¼šè¾¹ç•Œæƒ…å†µæµ‹è¯•')
console.log('='.repeat(70))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šç©ºè¾“å…¥
console.log('\n[æµ‹è¯•1] ç©ºè¾“å…¥: ""')
try {
  const code1 = ''
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  const parser1 = new TestParser(tokens1)
  const result1 = parser1.OptionalToken()
  
  if (result1 && parser1.tokenIndex === 0) {
    console.log('  âœ… æˆåŠŸï¼šOptionè§„åˆ™æ­£ç¡®å¤„ç†ç©ºè¾“å…¥')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šå•tokenè¾“å…¥
console.log('\n[æµ‹è¯•2] å•tokenè¾“å…¥: "x"')
try {
  const code2 = 'x'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  const parser2 = new TestParser(tokens2)
  const result2 = parser2.OptionalToken()
  
  if (result2 && parser2.tokenIndex === 1) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®å¤„ç†å•token')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šè¶…é•¿è¾“å…¥ï¼ˆ1000ä¸ªtokenï¼‰
console.log('\n[æµ‹è¯•3] è¶…é•¿è¾“å…¥: 1000ä¸ªæ ‡è¯†ç¬¦')
try {
  const identifiers = Array.from({ length: 1000 }, (_, i) => `x${i}`)
  const code3 = identifiers.join(' ')
  
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  console.log('  Tokenæ•°é‡:', tokens3.length)
  
  const parser3 = new TestParser(tokens3)
  const start = performance.now()
  const result3 = parser3.TokenList()
  const time = performance.now() - start
  
  if (result3 && parser3.tokenIndex === 1000) {
    console.log('  âœ… æˆåŠŸï¼šå¤„ç†1000ä¸ªtoken')
    console.log(`  è€—æ—¶: ${time.toFixed(2)}ms`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser3.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šæ·±åº¦åµŒå¥—ï¼ˆ50å±‚æ‹¬å·ï¼‰
console.log('\n[æµ‹è¯•4] æ·±åº¦åµŒå¥—: 50å±‚æ‹¬å· "(((...)))"')
try {
  const depth = 50
  const code4 = '('.repeat(depth) + 'x' + ')'.repeat(depth)
  
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  console.log('  åµŒå¥—æ·±åº¦:', depth)
  console.log('  Tokenæ•°é‡:', tokens4.length)
  
  const parser4 = new TestParser(tokens4)
  const start = performance.now()
  const result4 = parser4.NestedParens()
  const time = performance.now() - start
  
  if (result4 && parser4.tokenIndex === tokens4.length) {
    console.log('  âœ… æˆåŠŸï¼šå¤„ç†50å±‚åµŒå¥—')
    console.log(`  è€—æ—¶: ${time.toFixed(2)}ms`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•5ï¼šææ·±åµŒå¥—ï¼ˆ500å±‚ï¼‰- æµ‹è¯•æ ˆæº¢å‡ºä¿æŠ¤
console.log('\n[æµ‹è¯•5] ææ·±åµŒå¥—: 500å±‚æ‹¬å·ï¼ˆæµ‹è¯•æ ˆæº¢å‡ºï¼‰')
try {
  const depth = 500
  const code5 = '('.repeat(depth) + 'x' + ')'.repeat(depth)
  
  const lexer5 = new SubhutiLexer(testTokens)
  const tokens5 = lexer5.tokenize(code5)
  
  console.log('  åµŒå¥—æ·±åº¦:', depth)
  
  const parser5 = new TestParser(tokens5).cache(true)  // å¯ç”¨ç¼“å­˜å¸®åŠ©æ€§èƒ½
  const start = performance.now()
  const result5 = parser5.NestedParens()
  const time = performance.now() - start
  
  if (result5) {
    console.log('  âœ… æˆåŠŸï¼šå¤„ç†500å±‚åµŒå¥—ï¼ˆæ— æ ˆæº¢å‡ºï¼‰')
    console.log(`  è€—æ—¶: ${time.toFixed(2)}ms`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  if (e.message.includes('stack') || e.message.includes('recursion')) {
    console.log('  âš ï¸  è­¦å‘Šï¼šå‘ç”Ÿæ ˆæº¢å‡ºï¼ˆè¿™æ˜¯é¢„æœŸçš„è¾¹ç•Œæƒ…å†µï¼‰')
    console.log('  å»ºè®®ï¼šé™åˆ¶è¯­æ³•åµŒå¥—æ·±åº¦æˆ–ä½¿ç”¨è¿­ä»£æ–¹å¼')
    passed++  // ä¸ç®—å¤±è´¥ï¼Œå› ä¸ºææ·±åµŒå¥—æœ¬èº«å°±æ˜¯è¾¹ç•Œ
  } else {
    console.log('  âŒ å¼‚å¸¸:', e.message)
    failed++
  }
}

// æµ‹è¯•6ï¼šé•¿åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”çš„1000ä¸ªå…ƒç´ ï¼‰
console.log('\n[æµ‹è¯•6] é•¿åˆ—è¡¨: 1000ä¸ªé€—å·åˆ†éš”çš„æ ‡è¯†ç¬¦')
try {
  const identifiers = Array.from({ length: 1000 }, (_, i) => `x${i}`)
  const code6 = identifiers.join(', ')
  
  const lexer6 = new SubhutiLexer(testTokens)
  const tokens6 = lexer6.tokenize(code6)
  
  console.log('  å…ƒç´ æ•°é‡: 1000')
  console.log('  Tokenæ•°é‡:', tokens6.length)
  
  const parser6 = new TestParser(tokens6)
  const start = performance.now()
  const result6 = parser6.LongList()
  const time = performance.now() - start
  
  if (result6 && parser6.tokenIndex === 1999) {  // 1000ä¸ªæ ‡è¯†ç¬¦ + 999ä¸ªé€—å·
    console.log('  âœ… æˆåŠŸï¼šå¤„ç†é•¿åˆ—è¡¨')
    console.log(`  è€—æ—¶: ${time.toFixed(2)}ms`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser6.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•7ï¼šEOFè¾¹ç•Œï¼ˆæ¶ˆè´¹åˆ°æœ€åä¸€ä¸ªtokenï¼‰
console.log('\n[æµ‹è¯•7] EOFè¾¹ç•Œ: "x y z" (æ¶ˆè´¹æ‰€æœ‰token)')
try {
  const code7 = 'x y z'
  const lexer7 = new SubhutiLexer(testTokens)
  const tokens7 = lexer7.tokenize(code7)
  
  const parser7 = new TestParser(tokens7)
  const result7 = parser7.TokenList()
  
  // æ£€æŸ¥æ˜¯å¦æ¶ˆè´¹åˆ°EOF
  const atEOF = parser7.tokenIndex === tokens7.length
  
  if (result7 && atEOF) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®å¤„ç†EOFè¾¹ç•Œ')
    console.log('  At EOF:', atEOF)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•8ï¼šManyè§„åˆ™çš„ç©ºåŒ¹é…ï¼ˆ0æ¬¡ï¼‰
console.log('\n[æµ‹è¯•8] Manyè§„åˆ™ç©ºåŒ¹é…: ";" (0ä¸ªæ ‡è¯†ç¬¦)')
try {
  const code8 = ''
  const lexer8 = new SubhutiLexer(testTokens)
  const tokens8 = lexer8.tokenize(code8)
  
  const parser8 = new TestParser(tokens8)
  const result8 = parser8.TokenList()  // Manyå…è®¸0æ¬¡åŒ¹é…
  
  if (result8 && parser8.tokenIndex === 0) {
    console.log('  âœ… æˆåŠŸï¼šManyæ­£ç¡®å¤„ç†0æ¬¡åŒ¹é…')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(70))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(70))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(70))

console.log('\nğŸ“‹ è¾¹ç•Œæƒ…å†µè¦ç‚¹ï¼š')
console.log('1. ç©ºè¾“å…¥ï¼šOption/Manyè§„åˆ™æ­£ç¡®å¤„ç†')
console.log('2. å•tokenï¼šæœ€å°æœ‰æ•ˆè¾“å…¥')
console.log('3. è¶…é•¿è¾“å…¥ï¼šæ€§èƒ½æµ‹è¯•ï¼ˆ1000+ tokensï¼‰')
console.log('4. æ·±åº¦åµŒå¥—ï¼šé€’å½’è§„åˆ™ï¼ˆ50-500å±‚ï¼‰')
console.log('5. EOFè¾¹ç•Œï¼šæ­£ç¡®è¯†åˆ«è¾“å…¥ç»“æŸ')
console.log('6. æ ˆæº¢å‡ºä¿æŠ¤ï¼šææ·±åµŒå¥—çš„å¤„ç†')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}





















