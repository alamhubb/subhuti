/**
 * SubhutiParser æµ‹è¯• 014ï¼šé«˜çº§ç‰¹æ€§æµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. setTokens() - åŠ¨æ€æ›´æ–°tokenæµ
 * 2. å¤šæ¬¡è§£æï¼ˆå¤ç”¨parserï¼‰
 * 3. é“¾å¼è°ƒç”¨ï¼ˆcache + debug + errorHandlerï¼‰
 * 4. è§„åˆ™é€’å½’ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
 * 5. Unicode å’Œç‰¹æ®Šå­—ç¬¦å¤„ç†
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createRegToken, createValueRegToken, createKeywordToken } from "../../src/struct/SubhutiCreateToken"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  LetTok: createKeywordToken('LetTok', 'let'),
  Eq: createValueRegToken('Eq', /=/, '='),
  Plus: createValueRegToken('Plus', /\+/, '+'),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  LParen: createValueRegToken('LParen', /\(/, '('),
  RParen: createValueRegToken('RParen', /\)/, ')'),
  Identifier: createRegToken('Identifier', /[a-zA-Z_\u4e00-\u9fa5][a-zA-Z0-9_\u4e00-\u9fa5]*/),  // æ”¯æŒä¸­æ–‡
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  LetTok() { return this.consume(testTokensObj.LetTok) }
  Eq() { return this.consume(testTokensObj.Eq) }
  Plus() { return this.consume(testTokensObj.Plus) }
  Semicolon() { return this.consume(testTokensObj.Semicolon) }
  LParen() { return this.consume(testTokensObj.LParen) }
  RParen() { return this.consume(testTokensObj.RParen) }
  Identifier() { return this.consume(testTokensObj.Identifier) }
  Number() { return this.consume(testTokensObj.Number) }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  @SubhutiRule
  SimpleDeclaration() {
    this.tokenConsumer.LetTok()
    this.tokenConsumer.Identifier()
    this.tokenConsumer.Semicolon()
  }
  
  @SubhutiRule
  Expression() {
    this.tokenConsumer.Number()
    this.Many(() => {
      this.tokenConsumer.Plus()
      this.tokenConsumer.Number()
    })
  }
  
  // å¸¦æ‹¬å·çš„è¡¨è¾¾å¼ï¼ˆæµ‹è¯•é€’å½’ï¼‰
  @SubhutiRule
  ParenExpression() {
    this.Or([
      { alt: () => this.tokenConsumer.Number() },
      {
        alt: () => {
          this.tokenConsumer.LParen()
          this.ParenExpression()
          this.tokenConsumer.RParen()
        }
      }
    ])
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(70))
console.log('SubhutiParser æµ‹è¯• 014ï¼šé«˜çº§ç‰¹æ€§æµ‹è¯•')
console.log('='.repeat(70))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šsetTokens() - åŠ¨æ€æ›´æ–°tokenæµ
console.log('\n[æµ‹è¯•1] setTokens() - åŠ¨æ€æ›´æ–°tokenæµ')
try {
  const lexer = new SubhutiLexer(testTokens)
  const parser = new TestParser()
  
  // ç¬¬ä¸€æ¬¡è§£æ
  const code1 = 'let x ;'
  const tokens1 = lexer.tokenize(code1)
  parser.setTokens(tokens1)
  const result1 = parser.SimpleDeclaration()
  
  // ç¬¬äºŒæ¬¡è§£æï¼ˆå¤ç”¨parserï¼‰
  const code2 = 'let y ;'
  const tokens2 = lexer.tokenize(code2)
  parser.setTokens(tokens2)
  const result2 = parser.SimpleDeclaration()
  
  if (result1 && result2) {
    console.log('  âœ… æˆåŠŸï¼šsetTokens() æ­£ç¡®é‡ç½®çŠ¶æ€')
    console.log('  ç¬¬1æ¬¡è§£æ:', result1.name)
    console.log('  ç¬¬2æ¬¡è§£æ:', result2.name)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šsetTokens() æœªæ­£ç¡®é‡ç½®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šé“¾å¼è°ƒç”¨
console.log('\n[æµ‹è¯•2] é“¾å¼è°ƒç”¨: .cache(true).debug(false).errorHandler(true)')
try {
  const code = 'let x ;'
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  const parser = new TestParser(tokens)
    .cache(true)
    .debug(false)
    .errorHandler(true)
  
  const result = parser.SimpleDeclaration()
  
  if (result) {
    console.log('  âœ… æˆåŠŸï¼šé“¾å¼è°ƒç”¨æ­£ç¡®å·¥ä½œ')
    console.log('  Cache enabled:', parser.enableMemoization)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šè§„åˆ™é€’å½’ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
console.log('\n[æµ‹è¯•3] è§„åˆ™é€’å½’: "((1))"')
try {
  const code = '((1))'
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  const parser = new TestParser(tokens)
  const result = parser.ParenExpression()
  
  if (result && parser.tokenIndex === 5) {
    console.log('  âœ… æˆåŠŸï¼šé€’å½’è§„åˆ™æ­£ç¡®å·¥ä½œ')
    console.log('  æ¶ˆè´¹äº†', parser.tokenIndex, 'ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šUnicode å’Œä¸­æ–‡æ ‡è¯†ç¬¦
console.log('\n[æµ‹è¯•4] Unicodeæ ‡è¯†ç¬¦: "let å˜é‡ ;"')
try {
  const code = 'let å˜é‡ ;'
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  console.log('  Tokens:', tokens.map(t => `${t.tokenName}(${t.tokenValue})`).join(' '))
  
  const parser = new TestParser(tokens)
  const result = parser.SimpleDeclaration()
  
  if (result && parser.tokenIndex === 3) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®å¤„ç†Unicodeæ ‡è¯†ç¬¦')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•5ï¼šç‰¹æ®Šå­—ç¬¦ï¼ˆemojiç­‰ï¼‰
console.log('\n[æµ‹è¯•5] Emojiæ ‡è¯†ç¬¦: "let _ğŸ˜€ ;" (å¦‚æœæ­£åˆ™æ”¯æŒ)')
try {
  const code = 'let _test ;'  // ä½¿ç”¨æ™®é€šæ ‡è¯†ç¬¦ï¼Œå› ä¸ºé»˜è®¤æ­£åˆ™ä¸æ”¯æŒemoji
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  const parser = new TestParser(tokens)
  const result = parser.SimpleDeclaration()
  
  if (result) {
    console.log('  âœ… æˆåŠŸï¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦æ ‡è¯†ç¬¦')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âš ï¸  æ³¨æ„ï¼šé»˜è®¤æ­£åˆ™å¯èƒ½ä¸æ”¯æŒemoji')
  console.log('  æç¤ºï¼šå¯ä»¥è‡ªå®šä¹‰Identifieræ­£åˆ™æ¥æ”¯æŒ')
  passed++  // ä¸ç®—å¤±è´¥
}

// æµ‹è¯•6ï¼šç©ºç™½å­—ç¬¦çš„å„ç§å½¢å¼
console.log('\n[æµ‹è¯•6] å„ç§ç©ºç™½å­—ç¬¦: "let\\tx\\n;"')
try {
  const code = 'let\tx\n;'  // tabå’Œæ¢è¡Œç¬¦
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  console.log('  Tokenæ•°é‡:', tokens.length, 'ï¼ˆç©ºç™½å­—ç¬¦è¢«skipï¼‰')
  
  const parser = new TestParser(tokens)
  const result = parser.SimpleDeclaration()
  
  if (result && tokens.length === 3) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®å¤„ç†tabå’Œæ¢è¡Œç¬¦')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•7ï¼šç¼“å­˜æ¸…é™¤ï¼ˆsetTokensä¼šæ¸…é™¤ç¼“å­˜ï¼‰
console.log('\n[æµ‹è¯•7] ç¼“å­˜æ¸…é™¤: setTokens() åº”è¯¥æ¸…é™¤ç¼“å­˜')
try {
  const lexer = new SubhutiLexer(testTokens)
  const parser = new TestParser().cache(true)
  
  // ç¬¬ä¸€æ¬¡è§£æ
  const tokens1 = lexer.tokenize('1 + 2')
  parser.setTokens(tokens1)
  parser.Expression()
  
  // ç¬¬äºŒæ¬¡è§£æï¼ˆç¼“å­˜åº”è¯¥è¢«æ¸…é™¤ï¼‰
  const tokens2 = lexer.tokenize('3 + 4')
  parser.setTokens(tokens2)
  const result2 = parser.Expression()
  
  if (result2) {
    console.log('  âœ… æˆåŠŸï¼šsetTokens() æ­£ç¡®æ¸…é™¤ç¼“å­˜')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•8ï¼šç»„åˆæ‰€æœ‰åŠŸèƒ½
console.log('\n[æµ‹è¯•8] ç»„åˆåŠŸèƒ½: cache + errorHandler')
try {
  const code = 'let x ;'  // ä½¿ç”¨ç¬¦åˆ SimpleDeclaration çš„ä»£ç 
  const lexer = new SubhutiLexer(testTokens)
  const tokens = lexer.tokenize(code)
  
  const parser = new TestParser(tokens)
    .cache(true)
    .errorHandler(true)
  
  // ä¸å¯ç”¨ debugï¼Œé¿å…è¾“å‡ºå¹²æ‰°æµ‹è¯•
  const result = parser.SimpleDeclaration()
  
  if (result) {
    console.log('  âœ… æˆåŠŸï¼šæ‰€æœ‰åŠŸèƒ½ç»„åˆæ­£å¸¸å·¥ä½œ')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(70))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(70))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(70))

console.log('\nğŸ“‹ é«˜çº§ç‰¹æ€§è¦ç‚¹ï¼š')
console.log('1. setTokens() - åŠ¨æ€æ›´æ–°tokenæµï¼Œè‡ªåŠ¨æ¸…é™¤ç¼“å­˜')
console.log('2. é“¾å¼è°ƒç”¨ - cache/debug/errorHandler å¯ä»¥ç»„åˆä½¿ç”¨')
console.log('3. è§„åˆ™é€’å½’ - æ”¯æŒè‡ªé€’å½’è§„åˆ™ï¼ˆå¦‚æ‹¬å·è¡¨è¾¾å¼ï¼‰')
console.log('4. Unicodeæ”¯æŒ - å¯è‡ªå®šä¹‰æ­£åˆ™æ”¯æŒä¸­æ–‡ã€emojiç­‰')
console.log('5. å¤šæ¬¡è§£æ - å¯å¤ç”¨parserå®ä¾‹ï¼ˆé€šè¿‡setTokensï¼‰')
console.log('6. ç©ºç™½å¤„ç† - tabã€æ¢è¡Œç¬¦ç­‰è‡ªåŠ¨è¿‡æ»¤')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}

