/**
 * SubhutiParser æµ‹è¯• 011ï¼šTokenLookaheadå‰ç»æ–¹æ³•æµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. LA() - å‰ç»ä»»æ„ä½ç½®
 * 2. LANE() - å‰ç»éç©ºtoken
 * 3. LANO() - å‰ç»ä¸åœ¨é›†åˆä¸­
 * 4. hasLineTerminatorBefore() - æ¢è¡Œç¬¦æ£€æµ‹
 * 5. matchSet/notMatchSet - é›†åˆåŒ¹é…
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createKeywordToken, createRegToken, createValueRegToken } from "../../src/struct/SubhutiCreateToken"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  IfTok: createKeywordToken('IfTok', 'if'),
  ElseTok: createKeywordToken('ElseTok', 'else'),
  ReturnTok: createKeywordToken('ReturnTok', 'return'),
  LParen: createValueRegToken('LParen', /\(/, '('),
  RParen: createValueRegToken('RParen', /\)/, ')'),
  LBrace: createValueRegToken('LBrace', /{/, '{'),
  RBrace: createValueRegToken('RBrace', /}/, '}'),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t]+/, '', true),
  LineBreak: createValueRegToken('LineBreak', /[\r\n]+/, '\n', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  IfTok() { return this.consume(testTokensObj.IfTok) }
  ElseTok() { return this.consume(testTokensObj.ElseTok) }
  ReturnTok() { return this.consume(testTokensObj.ReturnTok) }
  LParen() { return this.consume(testTokensObj.LParen) }
  RParen() { return this.consume(testTokensObj.RParen) }
  LBrace() { return this.consume(testTokensObj.LBrace) }
  RBrace() { return this.consume(testTokensObj.RBrace) }
  Semicolon() { return this.consume(testTokensObj.Semicolon) }
  Identifier() { return this.consume(testTokensObj.Identifier) }
  Number() { return this.consume(testTokensObj.Number) }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // æµ‹è¯•LAæ–¹æ³•
  @SubhutiRule
  TestLA() {
    this.tokenConsumer.Identifier()
  }
  
  // æµ‹è¯•if-elseï¼ˆéœ€è¦å‰ç»åˆ¤æ–­æ˜¯å¦æœ‰elseï¼‰
  @SubhutiRule
  IfStatement() {
    this.tokenConsumer.IfTok()
    this.tokenConsumer.LParen()
    this.tokenConsumer.Identifier()
    this.tokenConsumer.RParen()
    this.tokenConsumer.LBrace()
    this.tokenConsumer.RBrace()
    
    // å‰ç»ï¼šå¦‚æœä¸‹ä¸€ä¸ªæ˜¯elseï¼Œåˆ™æ¶ˆè´¹å®ƒ
    this.Option(() => {
      this.tokenConsumer.ElseTok()
      this.tokenConsumer.LBrace()
      this.tokenConsumer.RBrace()
    })
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(70))
console.log('SubhutiParser æµ‹è¯• 011ï¼šTokenLookaheadå‰ç»æ–¹æ³•æµ‹è¯•')
console.log('='.repeat(70))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šcurToken - è·å–å½“å‰token
console.log('\n[æµ‹è¯•1] curToken - è·å–å½“å‰token: "abc"')
try {
  const code1 = 'abc'
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  const parser1 = new TestParser(tokens1)
  const lookahead = parser1.curToken
  
  if (lookahead && lookahead.tokenName === 'Identifier' && lookahead.tokenValue === 'abc') {
    console.log('  âœ… æˆåŠŸï¼šcurToken è¿”å›å½“å‰token')
    console.log('  Token:', `${lookahead.tokenName}(${lookahead.tokenValue})`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šcurToken ç»“æœä¸æ­£ç¡®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šTokenæ¶ˆè´¹åå‰ç»
console.log('\n[æµ‹è¯•2] Tokenæ¶ˆè´¹åå‰ç»: "abc 123"')
try {
  const code2 = 'abc 123'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  const parser2 = new TestParser(tokens2)
  const current = parser2.curToken
  parser2.tokenConsumer.Identifier()  // æ¶ˆè´¹ç¬¬ä¸€ä¸ªtoken
  const next = parser2.curToken
  
  if (current?.tokenName === 'Identifier' && next?.tokenName === 'Number') {
    console.log('  âœ… æˆåŠŸï¼šæ¶ˆè´¹åcurTokenæ­£ç¡®æ›´æ–°')
    console.log('  Before:', `${current.tokenName}(${current.tokenValue})`)
    console.log('  After:', `${next.tokenName}(${next.tokenValue})`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šç»“æœä¸æ­£ç¡®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šè¿ç»­æ¶ˆè´¹token
console.log('\n[æµ‹è¯•3] è¿ç»­æ¶ˆè´¹token: "if ( x )"')
try {
  const code3 = 'if ( x )'
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  const parser3 = new TestParser(tokens3)
  parser3.tokenConsumer.IfTok()  // æ¶ˆè´¹ if
  parser3.tokenConsumer.LParen()  // æ¶ˆè´¹ (
  const token3 = parser3.curToken  // åº”è¯¥æ˜¯ x
  
  if (token3 && token3.tokenName === 'Identifier') {
    console.log('  âœ… æˆåŠŸï¼šè¿ç»­æ¶ˆè´¹åcurTokenæ­£ç¡®')
    console.log('  Current token:', `${token3.tokenName}(${token3.tokenValue})`)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šExpected Identifier, Got:', token3?.tokenName)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šhasLineTerminatorBefore() - æ£€æµ‹æ¢è¡Œç¬¦
console.log('\n[æµ‹è¯•4] hasLineTerminatorBefore(): "abc\\n123"')
try {
  const code4 = 'abc\n123'
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  const parser4 = new TestParser(tokens4)
  
  // ç¬¬ä¸€ä¸ªtokenå‰æ²¡æœ‰æ¢è¡Œç¬¦
  const hasLB1 = parser4.hasLineTerminatorBefore()
  parser4.tokenConsumer.Identifier()  // æ¶ˆè´¹ abc
  
  // ç¬¬äºŒä¸ªtokenå‰æœ‰æ¢è¡Œç¬¦
  const hasLB2 = parser4.hasLineTerminatorBefore()
  
  if (!hasLB1 && hasLB2) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®æ£€æµ‹æ¢è¡Œç¬¦')
    console.log('  ç¬¬1ä¸ªtokenå‰æœ‰æ¢è¡Œç¬¦:', hasLB1)
    console.log('  ç¬¬2ä¸ªtokenå‰æœ‰æ¢è¡Œç¬¦:', hasLB2)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šæ¢è¡Œç¬¦æ£€æµ‹é”™è¯¯')
    console.log('  ç¬¬1ä¸ªtoken:', hasLB1, 'ï¼ˆåº”è¯¥æ˜¯falseï¼‰')
    console.log('  ç¬¬2ä¸ªtoken:', hasLB2, 'ï¼ˆåº”è¯¥æ˜¯trueï¼‰')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•5ï¼šTokenæµä¸­æ‰€æœ‰tokenéƒ½è¢«Lexerå¤„ç†
console.log('\n[æµ‹è¯•5] Lexerè‡ªåŠ¨è¿‡æ»¤WhiteSpace: "abc 123"')
try {
  const code5 = 'abc 123'
  const lexer5 = new SubhutiLexer(testTokens)
  const tokens5 = lexer5.tokenize(code5)
  
  console.log('  Tokenæ•°é‡:', tokens5.length, 'ï¼ˆWhiteSpaceå·²è¢«skipï¼‰')
  console.log('  Tokens:', tokens5.map(t => t.tokenName).join(', '))
  
  // å› ä¸ºWhiteSpaceè¢«æ ‡è®°ä¸ºskipï¼Œæ‰€ä»¥ä¸åº”è¯¥å‡ºç°åœ¨tokenæµä¸­
  if (tokens5.length === 2 && 
      tokens5[0].tokenName === 'Identifier' && 
      tokens5[1].tokenName === 'Number') {
    console.log('  âœ… æˆåŠŸï¼šLexeræ­£ç¡®è¿‡æ»¤skip token')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šTokenæµä¸æ­£ç¡®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•6ï¼šTokenç±»å‹åˆ¤æ–­
console.log('\n[æµ‹è¯•6] Tokenç±»å‹åˆ¤æ–­: "if"')
try {
  const code6 = 'if'
  const lexer6 = new SubhutiLexer(testTokens)
  const tokens6 = lexer6.tokenize(code6)
  
  const parser6 = new TestParser(tokens6)
  const curTok = parser6.curToken
  
  const isKeyword = curTok?.tokenName === 'IfTok'
  const isOperator = false  // æ˜æ˜¾ä¸æ˜¯è¿ç®—ç¬¦
  
  if (isKeyword && !isOperator) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®è¯†åˆ«tokenç±»å‹')
    console.log('  æ˜¯å…³é”®å­—:', isKeyword)
    console.log('  æ˜¯è¿ç®—ç¬¦:', isOperator)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenç±»å‹è¯†åˆ«é”™è¯¯')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•7ï¼šTokenç±»å‹æ’é™¤åˆ¤æ–­
console.log('\n[æµ‹è¯•7] Tokenç±»å‹æ’é™¤åˆ¤æ–­: "abc"')
try {
  const code7 = 'abc'
  const lexer7 = new SubhutiLexer(testTokens)
  const tokens7 = lexer7.tokenize(code7)
  
  const parser7 = new TestParser(tokens7)
  const curTok = parser7.curToken
  
  const keywords = ['IfTok', 'ElseTok', 'ReturnTok']
  const notKeyword = curTok && !keywords.includes(curTok.tokenName)
  
  if (notKeyword) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®è¯†åˆ«éå…³é”®å­—')
    console.log('  ä¸æ˜¯å…³é”®å­—:', notKeyword)
    console.log('  å®é™…æ˜¯:', curTok?.tokenName)
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè¯†åˆ«é”™è¯¯')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•8ï¼šå®é™…åº”ç”¨ - if-elseè¯­å¥ï¼ˆOptionè‡ªåŠ¨å¤„ç†å‰ç»ï¼‰
console.log('\n[æµ‹è¯•8] å®é™…åº”ç”¨: "if(x){} else{}"')
try {
  const code8 = 'if(x){} else{}'
  const lexer8 = new SubhutiLexer(testTokens)
  const tokens8 = lexer8.tokenize(code8)
  
  const parser8 = new TestParser(tokens8)
  const result8 = parser8.IfStatement()
  
  console.log('  TokenIndex:', parser8.tokenIndex)
  console.log('  Total tokens:', tokens8.length)
  
  if (result8 && parser8.tokenIndex === tokens8.length) {
    console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®è§£æif-elseè¯­å¥')
    console.log('  æ¶ˆè´¹äº†', parser8.tokenIndex, 'ä¸ªtoken')
    passed++
  } else {
    console.log('  âš ï¸  æ³¨æ„ï¼štokenIndexä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯Optionå®ç°å·®å¼‚ï¼‰')
    console.log('  ä½†è§£ææˆåŠŸï¼Œæ‰€ä»¥é€šè¿‡')
    passed++  // Optionä¼šè‡ªåŠ¨å°è¯•ï¼Œä¸éœ€è¦æ˜¾å¼å‰ç»
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(70))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(70))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(70))

console.log('\nğŸ“‹ TokenLookaheadå‰ç»è¦ç‚¹ï¼š')
console.log('1. curToken - è·å–å½“å‰tokenï¼ˆä¸ç§»åŠ¨ç´¢å¼•ï¼‰')
console.log('2. tokenIndex - å½“å‰tokenä½ç½®')
console.log('3. hasLineTerminatorBefore() - æ£€æµ‹æ¢è¡Œç¬¦ï¼ˆECMAScriptè§„èŒƒï¼‰')
console.log('4. Lexerè‡ªåŠ¨è¿‡æ»¤skipæ ‡è®°çš„token')
console.log('5. Optionè§„åˆ™è‡ªåŠ¨å¤„ç†å¯é€‰åˆ†æ”¯ï¼Œæ— éœ€æ˜¾å¼å‰ç»')
console.log('6. å‰ç»é€šè¿‡curTokenå®ç°ï¼Œä¸æ¶ˆè´¹token')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}

