/**
 * SubhutiParser æµ‹è¯• 013ï¼šCSTç»“æ„æµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. CSTèŠ‚ç‚¹ç»“æ„ï¼ˆname, children, value, locï¼‰
 * 2. Locationä¿¡æ¯å‡†ç¡®æ€§ï¼ˆstart/endä½ç½®ï¼‰
 * 3. åµŒå¥—CSTç»“æ„
 * 4. Token vs RuleèŠ‚ç‚¹åŒºåˆ†
 * 5. CSTè¾…åŠ©æ–¹æ³•ï¼ˆgetChild, getChildren, hasChildç­‰ï¼‰
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createKeywordToken, createRegToken, createValueRegToken } from "../../src/struct/SubhutiCreateToken"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  LetTok: createKeywordToken('LetTok', 'let'),
  Eq: createValueRegToken('Eq', /=/, '='),
  Plus: createValueRegToken('Plus', /\+/, '+'),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  LetTok() { return this.consume(testTokensObj.LetTok) }
  Eq() { return this.consume(testTokensObj.Eq) }
  Plus() { return this.consume(testTokensObj.Plus) }
  Semicolon() { return this.consume(testTokensObj.Semicolon) }
  Identifier() { return this.consume(testTokensObj.Identifier) }
  Number() { return this.consume(testTokensObj.Number) }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // å˜é‡å£°æ˜ï¼šlet x = 1 + 2 ;
  @SubhutiRule
  VariableDeclaration() {
    this.tokenConsumer.LetTok()
    this.tokenConsumer.Identifier()
    this.tokenConsumer.Eq()
    this.Expression()
    this.tokenConsumer.Semicolon()
  }
  
  // è¡¨è¾¾å¼ï¼šNumber ('+' Number)*
  @SubhutiRule
  Expression() {
    this.tokenConsumer.Number()
    this.Many(() => {
      this.tokenConsumer.Plus()
      this.tokenConsumer.Number()
    })
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(70))
console.log('SubhutiParser æµ‹è¯• 013ï¼šCSTç»“æ„æµ‹è¯•')
console.log('='.repeat(70))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šåŸºæœ¬CSTç»“æ„ï¼ˆä½¿ç”¨ debug('cst') å¯è§†åŒ–ï¼‰
console.log('\n[æµ‹è¯•1] åŸºæœ¬CSTç»“æ„: "let x = 1 ;"')
try {
  const code1 = 'let x = 1 ;'
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  const parser1 = new TestParser(tokens1).debug('cst')
  const cst1 = parser1.VariableDeclaration()
  
  console.log('  ') // CST ä¼šè‡ªåŠ¨è¾“å‡º
  
  if (cst1 && cst1.name === 'VariableDeclaration' && cst1.children) {
    console.log('  âœ… æˆåŠŸï¼šç”ŸæˆCST')
    console.log('  CST name:', cst1.name)
    console.log('  Childrenæ•°é‡:', cst1.children.length)
    console.log('  Children names:', cst1.children.map(c => c.name).join(', '))
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼šCSTç»“æ„ä¸æ­£ç¡®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šLocationä¿¡æ¯
console.log('\n[æµ‹è¯•2] Locationä¿¡æ¯: "let x = 1 ;"')
try {
  const code2 = 'let x = 1 ;'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  const parser2 = new TestParser(tokens2)
  const cst2 = parser2.VariableDeclaration()
  
  if (cst2 && cst2.loc) {
    console.log('  âœ… æˆåŠŸï¼šåŒ…å«Locationä¿¡æ¯')
    console.log('  Type:', cst2.loc.type)
    console.log('  Start:', `line ${cst2.loc.start.line}, column ${cst2.loc.start.column}, index ${cst2.loc.start.index}`)
    console.log('  End:', `line ${cst2.loc.end.line}, column ${cst2.loc.end.column}, index ${cst2.loc.end.index}`)
    
    // éªŒè¯ä½ç½®æ­£ç¡®æ€§
    if (cst2.loc.start.index === 0 && cst2.loc.end.index > 0) {
      console.log('  âœ… ä½ç½®ä¿¡æ¯å‡†ç¡®')
      passed++
    } else {
      console.log('  âŒ ä½ç½®ä¿¡æ¯ä¸å‡†ç¡®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šç¼ºå°‘Locationä¿¡æ¯')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šTokenèŠ‚ç‚¹ vs RuleèŠ‚ç‚¹
console.log('\n[æµ‹è¯•3] Token vs RuleèŠ‚ç‚¹åŒºåˆ†')
try {
  const code3 = 'let x = 1 ;'
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  const parser3 = new TestParser(tokens3)
  const cst3 = parser3.VariableDeclaration()
  
  if (cst3 && cst3.children) {
    const tokenNodes = cst3.children.filter(c => c.value !== undefined)
    const ruleNodes = cst3.children.filter(c => c.value === undefined && c.children !== undefined)
    
    console.log('  TokenèŠ‚ç‚¹æ•°:', tokenNodes.length, tokenNodes.map(t => `${t.name}(${t.value})`).join(', '))
    console.log('  RuleèŠ‚ç‚¹æ•°:', ruleNodes.length, ruleNodes.map(r => r.name).join(', '))
    
    if (tokenNodes.length > 0 && ruleNodes.length > 0) {
      console.log('  âœ… æˆåŠŸï¼šæ­£ç¡®åŒºåˆ†Tokenå’ŒRuleèŠ‚ç‚¹')
      passed++
    } else {
      console.log('  âŒ å¤±è´¥ï¼šèŠ‚ç‚¹ç±»å‹ä¸æ­£ç¡®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šCSTç»“æ„ä¸æ­£ç¡®')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šåµŒå¥—CSTç»“æ„ï¼ˆä½¿ç”¨ debug('cst') å¯è§†åŒ–ï¼‰
console.log('\n[æµ‹è¯•4] åµŒå¥—CST: "let x = 1 + 2 + 3 ;"')
try {
  const code4 = 'let x = 1 + 2 + 3 ;'
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  const parser4 = new TestParser(tokens4).debug('cst')
  const cst4 = parser4.VariableDeclaration()
  
  console.log('  ') // CST ä¼šè‡ªåŠ¨è¾“å‡º
  
  if (cst4) {
    // æŸ¥æ‰¾Expressionå­èŠ‚ç‚¹
    const expression = cst4.children?.find(c => c.name === 'Expression')
    
    if (expression && expression.children) {
      console.log('  âœ… æˆåŠŸï¼šåŒ…å«åµŒå¥—çš„ExpressionèŠ‚ç‚¹')
      console.log('  Expression children:', expression.children.length)
      console.log('  Expressionå†…å®¹:', expression.children.map(c => `${c.name}(${c.value || ''})`).join(' '))
      passed++
    } else {
      console.log('  âŒ å¤±è´¥ï¼šç¼ºå°‘Expressionå­èŠ‚ç‚¹')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•5ï¼šCSTè¾…åŠ©æ–¹æ³• - hasChild
console.log('\n[æµ‹è¯•5] CSTè¾…åŠ©æ–¹æ³• - hasChild()')
try {
  const code5 = 'let x = 1 ;'
  const lexer5 = new SubhutiLexer(testTokens)
  const tokens5 = lexer5.tokenize(code5)
  
  const parser5 = new TestParser(tokens5)
  const cst5 = parser5.VariableDeclaration()
  
  if (cst5) {
    const hasExpression = cst5.hasChild && cst5.hasChild('Expression')
    const hasStatement = cst5.hasChild && cst5.hasChild('Statement')
    
    console.log('  Has Expression:', hasExpression)
    console.log('  Has Statement:', hasStatement)
    
    if (hasExpression && !hasStatement) {
      console.log('  âœ… æˆåŠŸï¼šhasChild()æ­£ç¡®å·¥ä½œ')
      passed++
    } else {
      console.log('  âŒ å¤±è´¥ï¼šhasChild()ç»“æœä¸æ­£ç¡®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•6ï¼šCSTè¾…åŠ©æ–¹æ³• - getChild
console.log('\n[æµ‹è¯•6] CSTè¾…åŠ©æ–¹æ³• - getChild()')
try {
  const code6 = 'let x = 1 ;'
  const lexer6 = new SubhutiLexer(testTokens)
  const tokens6 = lexer6.tokenize(code6)
  
  const parser6 = new TestParser(tokens6)
  const cst6 = parser6.VariableDeclaration()
  
  if (cst6) {
    const firstLet = cst6.getChild && cst6.getChild('LetTok', 0)
    const firstIdentifier = cst6.getChild && cst6.getChild('Identifier', 0)
    
    console.log('  First LetTok:', firstLet ? `${firstLet.name}(${firstLet.value})` : 'null')
    console.log('  First Identifier:', firstIdentifier ? `${firstIdentifier.name}(${firstIdentifier.value})` : 'null')
    
    if (firstLet && firstLet.value === 'let' && firstIdentifier && firstIdentifier.value === 'x') {
      console.log('  âœ… æˆåŠŸï¼šgetChild()æ­£ç¡®å·¥ä½œ')
      passed++
    } else {
      console.log('  âŒ å¤±è´¥ï¼šgetChild()ç»“æœä¸æ­£ç¡®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•7ï¼šCSTè¾…åŠ©æ–¹æ³• - getChildren
console.log('\n[æµ‹è¯•7] CSTè¾…åŠ©æ–¹æ³• - getChildren()')
try {
  const code7 = 'let x = 1 + 2 ;'
  const lexer7 = new SubhutiLexer(testTokens)
  const tokens7 = lexer7.tokenize(code7)
  
  const parser7 = new TestParser(tokens7)
  const cst7 = parser7.VariableDeclaration()
  
  if (cst7) {
    const expression = cst7.getChild && cst7.getChild('Expression', 0)
    if (expression) {
      const numbers = expression.getChildren && expression.getChildren('Number')
      
      console.log('  Numbers:', numbers ? numbers.map(n => n.value).join(', ') : 'null')
      
      if (numbers && numbers.length === 2) {
        console.log('  âœ… æˆåŠŸï¼šgetChildren()æ­£ç¡®å·¥ä½œ')
        passed++
      } else {
        console.log('  âŒ å¤±è´¥ï¼šgetChildren()ç»“æœä¸æ­£ç¡®')
        failed++
      }
    } else {
      console.log('  âŒ å¤±è´¥ï¼šæ‰¾ä¸åˆ°Expression')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•8ï¼šç©ºchildrenä¼˜åŒ–
console.log('\n[æµ‹è¯•8] ç©ºchildrenä¼˜åŒ–ï¼ˆåº”è¯¥è¢«è®¾ç½®ä¸ºundefinedï¼‰')
try {
  const code8 = 'let x = 1 ;'
  const lexer8 = new SubhutiLexer(testTokens)
  const tokens8 = lexer8.tokenize(code8)
  
  const parser8 = new TestParser(tokens8)
  const cst8 = parser8.VariableDeclaration()
  
  if (cst8) {
    // TokenèŠ‚ç‚¹çš„childrenåº”è¯¥æ˜¯undefined
    const letToken = cst8.getChild && cst8.getChild('LetTok', 0)
    
    if (letToken && letToken.children === undefined) {
      console.log('  âœ… æˆåŠŸï¼šTokenèŠ‚ç‚¹çš„childrenè¢«ä¼˜åŒ–ä¸ºundefined')
      passed++
    } else {
      console.log('  âš ï¸  æ³¨æ„ï¼šTokenèŠ‚ç‚¹çš„childrenæœªä¼˜åŒ–:', letToken?.children)
      passed++  // ä¸ç®—å¤±è´¥ï¼Œè¿™æ˜¯ä¼˜åŒ–ï¼Œä¸æ˜¯å¿…é¡»
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šè§£æå¤±è´¥')
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(70))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(70))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(70))

console.log('\nğŸ“‹ CSTç»“æ„è¦ç‚¹ï¼š')
console.log('1. CSTèŠ‚ç‚¹åŒ…å«ï¼šname, children, value, loc')
console.log('2. TokenèŠ‚ç‚¹æœ‰valueï¼ŒRuleèŠ‚ç‚¹æœ‰children')
console.log('3. Locationä¿¡æ¯å‡†ç¡®åˆ°è¡Œåˆ—å·å’Œç´¢å¼•')
console.log('4. æ”¯æŒåµŒå¥—CSTç»“æ„')
console.log('5. è¾…åŠ©æ–¹æ³•ï¼šhasChild, getChild, getChildren, childCountç­‰')
console.log('6. ç©ºchildrenä¼˜åŒ–ä¸ºundefinedï¼ˆå‡å°‘å†…å­˜ï¼‰')
console.log('\nğŸ“‹ CST DebugåŠŸèƒ½ï¼š')
console.log('1. .debug(\'cst\') - è¾“å‡º CST æ ‘å½¢ç»“æ„')
console.log('2. è‡ªåŠ¨åœ¨è§£æå®Œæˆåè¾“å‡ºCST')
console.log('3. TokenèŠ‚ç‚¹æ˜¾ç¤ºä½ç½®ä¿¡æ¯')
console.log('4. æ¸…æ™°åŒºåˆ†Tokenå’ŒRuleèŠ‚ç‚¹')
console.log('5. ç”¨äºå¿«é€ŸéªŒè¯CSTç»“æ„æ­£ç¡®æ€§')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}

