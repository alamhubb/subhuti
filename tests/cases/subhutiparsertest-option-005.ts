/**
 * SubhutiParser æµ‹è¯• 005ï¼šOptionè§„åˆ™æµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. OptionåŒ¹é…æˆåŠŸçš„æƒ…å†µ
 * 2. OptionåŒ¹é…å¤±è´¥çš„æƒ…å†µï¼ˆä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
 * 3. Optionä¸å›ºå®štokençš„ç»„åˆ
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createKeywordToken, createRegToken, createValueRegToken, SubhutiCreateTokenGroupType } from "../../src/struct/SubhutiCreateToken.ts"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken.ts"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  AsTok: createKeywordToken('AsTok', 'as'),
  Eq: createValueRegToken('Eq', /=/, '='),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', 'skip'),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  AsTok() {
    return this.consume(testTokensObj.AsTok)
  }
  
  Eq() {
    return this.consume(testTokensObj.Eq)
  }
  
  Semicolon() {
    return this.consume(testTokensObj.Semicolon)
  }
  
  Identifier() {
    return this.consume(testTokensObj.Identifier)
  }
  
  Number() {
    return this.consume(testTokensObj.Number)
  }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // Identifier [as Identifier]
  @SubhutiRule
  ImportName() {
    this.tokenConsumer.Identifier()
    this.Option(() => {
      this.tokenConsumer.AsTok()
      this.tokenConsumer.Identifier()
    })
  }
  
  // Identifier [= Number]
  @SubhutiRule
  VariableDeclaration() {
    this.tokenConsumer.Identifier()
    this.Option(() => {
      this.tokenConsumer.Eq()
      this.tokenConsumer.Number()
    })
  }
  
  // Identifier [= Number] ;
  @SubhutiRule
  VariableStatement() {
    this.tokenConsumer.Identifier()
    this.Option(() => {
      this.tokenConsumer.Eq()
      this.tokenConsumer.Number()
    })
    this.tokenConsumer.Semicolon()
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(60))
console.log('SubhutiParser æµ‹è¯• 005ï¼šOptionè§„åˆ™æµ‹è¯•')
console.log('='.repeat(60))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šOptionåŒ¹é…æˆåŠŸ
console.log('\n[æµ‹è¯•1] OptionåŒ¹é…æˆåŠŸ: "name as userName"')
try {
  const code1 = 'name as userName'
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  console.log('  Token:', tokens1.map(t => t.tokenValue).join(' '))
  
  const parser1 = new TestParser(tokens1)
  const result1 = parser1.ImportName()
  
  if (result1 && parser1.tokenIndex === 3) {
    console.log('  âœ… æˆåŠŸï¼šOptionåŒ¹é…æˆåŠŸï¼Œæ¶ˆè´¹äº†3ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser1.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•2ï¼šOptionåŒ¹é…å¤±è´¥ï¼ˆä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
console.log('\n[æµ‹è¯•2] OptionåŒ¹é…å¤±è´¥: "name" (æ²¡æœ‰aséƒ¨åˆ†)')
try {
  const code2 = 'name'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  console.log('  Token:', tokens2.map(t => t.tokenValue).join(' '))
  
  const parser2 = new TestParser(tokens2)
  const result2 = parser2.ImportName()
  
  if (result2 && parser2.tokenIndex === 1) {
    console.log('  âœ… æˆåŠŸï¼šOptionå¤±è´¥ä¸æŠ›å¼‚å¸¸ï¼Œåªæ¶ˆè´¹äº†å¿…éœ€çš„éƒ¨åˆ†')
    console.log('  æ¶ˆè´¹äº†', parser2.tokenIndex, 'ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser2.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•3ï¼šOptionåè·Ÿå›ºå®štoken - åŒ¹é…Option
console.log('\n[æµ‹è¯•3] Optionåè·Ÿå›ºå®štoken: "x = 10 ;"')
try {
  const code3 = 'x = 10 ;'
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  console.log('  Token:', tokens3.map(t => t.tokenValue).join(' '))
  console.log('  Tokenæ•°é‡:', tokens3.length)
  
  const parser3 = new TestParser(tokens3)
  const result3 = parser3.VariableStatement()
  
  if (result3 && parser3.tokenIndex === 4) {
    console.log('  âœ… æˆåŠŸï¼šOptionåŒ¹é…ï¼Œæ¶ˆè´¹äº†4ä¸ªtoken')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser3.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•4ï¼šOptionåè·Ÿå›ºå®štoken - ä¸åŒ¹é…Option
console.log('\n[æµ‹è¯•4] Optionåè·Ÿå›ºå®štoken: "x ;" (æ²¡æœ‰åˆå§‹åŒ–)')
try {
  const code4 = 'x ;'
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  console.log('  Token:', tokens4.map(t => t.tokenValue).join(' '))
  
  const parser4 = new TestParser(tokens4)
  const result4 = parser4.VariableStatement()
  
  if (result4 && parser4.tokenIndex === 2) {
    console.log('  âœ… æˆåŠŸï¼šOptionä¸åŒ¹é…ï¼Œæ­£ç¡®æ¶ˆè´¹äº†åˆ†å·')
    console.log('  æ¶ˆè´¹äº†', parser4.tokenIndex, 'ä¸ªtoken (x + ;)')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser4.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// æµ‹è¯•5ï¼šOptionéƒ¨åˆ†åŒ¹é…å¤±è´¥ï¼ˆå›æº¯ï¼‰
console.log('\n[æµ‹è¯•5] Optionéƒ¨åˆ†åŒ¹é…å¤±è´¥: "x = abc" (æœŸæœ›Number)')
try {
  const code5 = 'x = abc'
  const lexer5 = new SubhutiLexer(testTokens)
  const tokens5 = lexer5.tokenize(code5)
  
  console.log('  Token:', tokens5.map(t => `${t.tokenName}:${t.tokenValue}`).join(' '))
  console.log('  Optionå°è¯•ï¼šEq (âœ…) + Number (âŒ å®é™…æ˜¯Identifier)')
  console.log('  é¢„æœŸï¼šOptionæ•´ä½“å¤±è´¥ï¼Œå›æº¯')
  
  const parser5 = new TestParser(tokens5)
  const result5 = parser5.VariableDeclaration()
  
  if (result5 && parser5.tokenIndex === 1) {
    console.log('  âœ… æˆåŠŸï¼šOptionéƒ¨åˆ†åŒ¹é…å¤±è´¥ï¼Œæ­£ç¡®å›æº¯')
    console.log('  æ¶ˆè´¹äº†', parser5.tokenIndex, 'ä¸ªtoken (åªæœ‰x)')
    console.log('  å‰©ä½™ token:', tokens5.length - parser5.tokenIndex, 'ä¸ª (= abc)')
    passed++
  } else {
    console.log('  âŒ å¤±è´¥ï¼štokenIndex =', parser5.tokenIndex)
    failed++
  }
} catch (e: any) {
  console.log('  âŒ å¼‚å¸¸:', e.message)
  failed++
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(60))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(60))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(60))

console.log('\nğŸ“‹ Optionè§„åˆ™è¦ç‚¹ï¼š')
console.log('1. OptionåŒ¹é…0æ¬¡æˆ–1æ¬¡ï¼ˆå¯é€‰éƒ¨åˆ†ï¼‰')
console.log('2. Optionå¤±è´¥ä¸æŠ›å‡ºå¼‚å¸¸')
console.log('3. Optionéƒ¨åˆ†åŒ¹é…å¤±è´¥ä¼šå›æº¯')
console.log('4. Optionæ¯”Oræ›´æ¸…æ™°åœ°è¡¨è¾¾å¯é€‰è¯­ä¹‰')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}


