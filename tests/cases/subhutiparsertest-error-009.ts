/**
 * SubhutiParser æµ‹è¯• 009ï¼šé”™è¯¯å¤„ç†æµ‹è¯•
 * 
 * æµ‹è¯•ç›®æ ‡ï¼š
 * 1. é”™è¯¯ä¿¡æ¯æ ¼å¼
 * 2. ruleStack è¿½è¸ª
 * 3. ä½ç½®ä¿¡æ¯å‡†ç¡®æ€§
 * 4. æ™ºèƒ½ä¿®å¤å»ºè®®
 * 5. è¯¦ç»†/ç®€æ´æ¨¡å¼åˆ‡æ¢
 */

import SubhutiLexer from "../../src/SubhutiLexer.ts"
import SubhutiParser, { Subhuti, SubhutiRule } from "../../src/SubhutiParser.ts"
import SubhutiTokenConsumer from "../../src/SubhutiTokenConsumer.ts"
import { createKeywordToken, createRegToken, createValueRegToken } from "../../src/struct/SubhutiCreateToken"
import type { SubhutiTokenConsumerConstructor } from "../../src/SubhutiParser.ts"
import SubhutiMatchToken from "../../src/struct/SubhutiMatchToken"
import { ParsingError } from "../../src/SubhutiError.ts"

// ============================================
// å®šä¹‰Tokené›†
// ============================================

const testTokensObj = {
  LetTok: createKeywordToken('LetTok', 'let'),
  VarTok: createKeywordToken('VarTok', 'var'),
  LBrace: createValueRegToken('LBrace', /{/, '{'),
  RBrace: createValueRegToken('RBrace', /}/, '}'),
  LParen: createValueRegToken('LParen', /\(/, '('),
  RParen: createValueRegToken('RParen', /\)/, ')'),
  Semicolon: createValueRegToken('Semicolon', /;/, ';'),
  Eq: createValueRegToken('Eq', /=/, '='),
  Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
  Number: createRegToken('Number', /[0-9]+/),
  WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', true),
}

const testTokens = Object.values(testTokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
  LetTok() { return this.consume(testTokensObj.LetTok) }
  VarTok() { return this.consume(testTokensObj.VarTok) }
  LBrace() { return this.consume(testTokensObj.LBrace) }
  RBrace() { return this.consume(testTokensObj.RBrace) }
  LParen() { return this.consume(testTokensObj.LParen) }
  RParen() { return this.consume(testTokensObj.RParen) }
  Semicolon() { return this.consume(testTokensObj.Semicolon) }
  Eq() { return this.consume(testTokensObj.Eq) }
  Identifier() { return this.consume(testTokensObj.Identifier) }
  Number() { return this.consume(testTokensObj.Number) }
}

// ============================================
// æµ‹è¯•Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
  constructor(
    tokens?: SubhutiMatchToken[],
    TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
  ) {
    super(tokens, TokenConsumerClass)
  }
  
  // ç®€å•å£°æ˜
  @SubhutiRule
  SimpleDeclaration() {
    this.tokenConsumer.LetTok()
    this.tokenConsumer.Identifier()
    this.tokenConsumer.Semicolon()
  }
  
  // å—è¯­å¥ï¼ˆæµ‹è¯•åµŒå¥—è§„åˆ™æ ˆï¼‰
  @SubhutiRule
  BlockStatement() {
    this.tokenConsumer.LBrace()
    this.StatementList()
    this.tokenConsumer.RBrace()
  }
  
  @SubhutiRule
  StatementList() {
    this.Many(() => this.Statement())
  }
  
  @SubhutiRule
  Statement() {
    this.SimpleDeclaration()
  }
  
  // å‡½æ•°è°ƒç”¨ï¼ˆæµ‹è¯•æ‹¬å·åŒ¹é…ï¼‰
  @SubhutiRule
  FunctionCall() {
    this.tokenConsumer.Identifier()
    this.tokenConsumer.LParen()
    this.ArgumentList()
    this.tokenConsumer.RParen()
  }
  
  @SubhutiRule
  ArgumentList() {
    this.Many(() => this.tokenConsumer.Identifier())
  }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('='.repeat(70))
console.log('SubhutiParser æµ‹è¯• 009ï¼šé”™è¯¯å¤„ç†æµ‹è¯•')
console.log('='.repeat(70))

let passed = 0
let failed = 0

// æµ‹è¯•1ï¼šåŸºæœ¬é”™è¯¯ä¿¡æ¯
console.log('\n[æµ‹è¯•1] åŸºæœ¬é”™è¯¯ä¿¡æ¯: "let x" (ç¼ºå°‘åˆ†å·)')
try {
  const code1 = 'let x'
  const lexer1 = new SubhutiLexer(testTokens)
  const tokens1 = lexer1.tokenize(code1)
  
  const parser1 = new TestParser(tokens1).errorHandler(true)
  parser1.SimpleDeclaration()
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  Expected:', e.expected)
    console.log('  Found:', e.found?.tokenName || 'EOF')
    console.log('  Position:', `line ${e.position.line}, column ${e.position.column}`)
    
    if (e.expected === 'Semicolon' && e.position.line === 1) {
      console.log('  âœ… é”™è¯¯ä¿¡æ¯å‡†ç¡®')
      passed++
    } else {
      console.log('  âŒ é”™è¯¯ä¿¡æ¯ä¸å‡†ç¡®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError:', e.message)
    failed++
  }
}

// æµ‹è¯•2ï¼šruleStack è¿½è¸ªï¼ˆåµŒå¥—è§„åˆ™ï¼‰
console.log('\n[æµ‹è¯•2] RuleStack è¿½è¸ª: "{ let x }" (ç¼ºå°‘åˆ†å·ï¼ŒåµŒå¥—åœ¨Blockä¸­)')
try {
  const code2 = '{ let x }'
  const lexer2 = new SubhutiLexer(testTokens)
  const tokens2 = lexer2.tokenize(code2)
  
  const parser2 = new TestParser(tokens2).errorHandler(true)
  parser2.BlockStatement()
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  RuleStack:', e.ruleStack.join(' > '))
    
    // éªŒè¯è§„åˆ™æ ˆåŒ…å«åµŒå¥—ä¿¡æ¯ï¼ˆè‡³å°‘åŒ…å«BlockStatementï¼‰
    if (e.ruleStack.length > 0 && e.ruleStack.includes('BlockStatement')) {
      console.log('  âœ… RuleStack æ­£ç¡®è¿½è¸ªï¼ˆåŒ…å«BlockStatementï¼‰')
      console.log('  Note: RuleStackæ·±åº¦:', e.ruleStack.length)
      passed++
    } else {
      console.log('  âŒ RuleStack ä¸å®Œæ•´ï¼ŒStack:', e.ruleStack)
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError')
    failed++
  }
}

// æµ‹è¯•3ï¼šä½ç½®ä¿¡æ¯å‡†ç¡®æ€§ï¼ˆå¤šè¡Œä»£ç ï¼‰
console.log('\n[æµ‹è¯•3] å¤šè¡Œä½ç½®ä¿¡æ¯: "let x;\\nlet y" (ç¬¬äºŒè¡Œç¼ºå°‘åˆ†å·)')
try {
  const code3 = 'let x;\nlet y'
  const lexer3 = new SubhutiLexer(testTokens)
  const tokens3 = lexer3.tokenize(code3)
  
  const parser3 = new TestParser(tokens3).errorHandler(true)
  parser3.SimpleDeclaration()  // ç¬¬ä¸€ä¸ªæˆåŠŸ
  parser3.SimpleDeclaration()  // ç¬¬äºŒä¸ªå¤±è´¥
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  Position:', `line ${e.position.line}, column ${e.position.column}`)
    
    if (e.position.line === 2) {
      console.log('  âœ… ä½ç½®ä¿¡æ¯å‡†ç¡®ï¼ˆç¬¬2è¡Œï¼‰')
      passed++
    } else {
      console.log('  âŒ ä½ç½®ä¿¡æ¯é”™è¯¯ï¼ˆåº”è¯¥æ˜¯ç¬¬2è¡Œï¼‰')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError')
    failed++
  }
}

// æµ‹è¯•4ï¼šæ™ºèƒ½ä¿®å¤å»ºè®®ï¼ˆç¼ºå°‘é—­åˆç¬¦å·ï¼‰
console.log('\n[æµ‹è¯•4] æ™ºèƒ½å»ºè®®: "func(" (ç¼ºå°‘å³æ‹¬å·)')
try {
  const code4 = 'func('
  const lexer4 = new SubhutiLexer(testTokens)
  const tokens4 = lexer4.tokenize(code4)
  
  const parser4 = new TestParser(tokens4).errorHandler(true)
  parser4.FunctionCall()
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  Suggestions:', e.suggestions)
    
    if (e.suggestions && e.suggestions.length > 0) {
      console.log('  âœ… æä¾›äº†æ™ºèƒ½ä¿®å¤å»ºè®®')
      passed++
    } else {
      console.log('  âš ï¸  æœªæä¾›å»ºè®®ï¼ˆå¯èƒ½ä¸æ˜¯å¸¸è§é”™è¯¯åœºæ™¯ï¼‰')
      passed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError')
    failed++
  }
}

// æµ‹è¯•5ï¼šç®€æ´æ¨¡å¼ï¼ˆä¸ç”Ÿæˆå»ºè®®ï¼‰
console.log('\n[æµ‹è¯•5] ç®€æ´æ¨¡å¼: "let x" (ä¸ç”Ÿæˆæ™ºèƒ½å»ºè®®)')
try {
  const code5 = 'let x'
  const lexer5 = new SubhutiLexer(testTokens)
  const tokens5 = lexer5.tokenize(code5)
  
  const parser5 = new TestParser(tokens5).errorHandler(false)  // å…³é—­è¯¦ç»†æ¨¡å¼
  parser5.SimpleDeclaration()
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  Suggestions length:', e.suggestions.length)
    
    if (e.suggestions.length === 0) {
      console.log('  âœ… ç®€æ´æ¨¡å¼ä¸ç”Ÿæˆå»ºè®®')
      passed++
    } else {
      console.log('  âŒ ç®€æ´æ¨¡å¼ä»ç„¶ç”Ÿæˆäº†å»ºè®®')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError')
    failed++
  }
}

// æµ‹è¯•6ï¼šEOF é”™è¯¯
console.log('\n[æµ‹è¯•6] EOF é”™è¯¯: "" (ç©ºè¾“å…¥)')
try {
  const code6 = ''
  const lexer6 = new SubhutiLexer(testTokens)
  const tokens6 = lexer6.tokenize(code6)
  
  const parser6 = new TestParser(tokens6).errorHandler(true)
  parser6.SimpleDeclaration()
  
  console.log('  âŒ å¤±è´¥ï¼šåº”è¯¥æŠ›å‡ºå¼‚å¸¸')
  failed++
} catch (e: any) {
  if (e instanceof ParsingError) {
    console.log('  âœ… æˆåŠŸï¼šæŠ›å‡º ParsingError')
    console.log('  Found:', e.found?.tokenName || 'EOF')
    
    if (!e.found || e.found.tokenName === undefined) {
      console.log('  âœ… æ­£ç¡®è¯†åˆ«EOFé”™è¯¯')
      passed++
    } else {
      console.log('  âŒ EOFè¯†åˆ«é”™è¯¯')
      failed++
    }
  } else {
    console.log('  âŒ å¤±è´¥ï¼šä¸æ˜¯ ParsingError')
    failed++
  }
}

// ============================================
// æµ‹è¯•æ€»ç»“
// ============================================

console.log('\n' + '='.repeat(70))
console.log('æµ‹è¯•æ€»ç»“')
console.log('='.repeat(70))
console.log(`é€šè¿‡: ${passed}/${passed + failed}`)
console.log(`å¤±è´¥: ${failed}/${passed + failed}`)
console.log('='.repeat(70))

console.log('\nğŸ“‹ é”™è¯¯å¤„ç†è¦ç‚¹ï¼š')
console.log('1. ParsingError åŒ…å« expectedã€foundã€positionã€ruleStack')
console.log('2. RuleStack è¿½è¸ªå®Œæ•´çš„è§„åˆ™è°ƒç”¨é“¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰')
console.log('3. ä½ç½®ä¿¡æ¯å‡†ç¡®åˆ°è¡Œåˆ—å·')
console.log('4. æ™ºèƒ½å»ºè®®è¦†ç›–å¸¸è§é”™è¯¯åœºæ™¯ï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰')
console.log('5. ç®€æ´æ¨¡å¼é€‚åˆç”Ÿäº§ç¯å¢ƒï¼ˆä¸ç”Ÿæˆå»ºè®®ï¼‰')

if (failed === 0) {
  console.log('\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
  process.exit(0)
} else {
  console.log('\nâŒ æœ‰æµ‹è¯•å¤±è´¥')
  process.exit(1)
}

